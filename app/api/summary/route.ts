// app/api/summary/route.ts (ì„œë²„ ì „ìš© API Route)
import { summaryParser } from "@/utils/api/analysis-utils";
import { createClient } from "@/utils/supabase/server";
import { cookies } from "next/headers";
import { NextResponse } from "next/server";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY, // í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë…¸ì¶œë˜ì§€ ì•ŠëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
});

export async function POST(req: Request) {
  const cookiesStore = await cookies();
  const supabase = await createClient(cookiesStore);
  const user = await supabase.auth.getUser();
  if (!user.data) {
    console.error("ë¡œê·¸ì¸ë˜ì§€ ì•Šì€ ì‚¬ìš©ì:");
    return NextResponse.json(
      { error: "ì‚¬ìš©ì ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨" },
      { status: 500 }
    );
  }

  try {
    const { title, body } = await req.json();

    if (!title || !body) {
      return NextResponse.json(
        { error: "titleê³¼ bodyê°€ í•„ìš”í•©ë‹ˆë‹¤." },
        { status: 400 }
      );
    }

    const prompt = `ì œëª©: ${title}\n\në³¸ë¬¸: ${body}`;

    const summaryResponse = await openai.chat.completions.create({
      model: "chatgpt-4o-latest",
      messages: [
        {
          role: "system",
          content: `The response must be **strictly** limited to ${
            ~~((MAX_TOKENS * 0.9) / 100) * 100
          } tokens or fewer. Keep the answer concise and within the specified token limit.`,
        },
        {
          role: "system",
          content: `ë‹¹ì‹ ì€ ê°œë°œìë¥¼ ìœ„í•œ AI ë©˜í† ì…ë‹ˆë‹¤. ìœ ì €ë¡œë¶€í„° ê¸°ìˆ ë¸”ë¡œê·¸ì˜ ê²Œì‹œê¸€ì„ ì „ë‹¬ë°›ìœ¼ë©´, ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ê³µë¶€í•  ê±°ë¦¬ë¥¼ ì¶”ì²œí•´ì¤ë‹ˆë‹¤. ë‹¤ìŒì˜ ì˜ˆì‹œì™€ ê°™ì´ ì‘ì„±í•˜ì—¬ ë°˜í™˜í•˜ì—¬ì£¼ì„¸ìš”.`,
        },
        { role: "system", content: MARKDOWN_SUMMARY },
        { role: "user", content: prompt },
      ],
      max_tokens: MAX_TOKENS,
    });

    const summary =
      summaryResponse.choices[0]?.message?.content?.trim() ||
      "ìš”ì•½ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.";

    // ğŸ“Œ 2ï¸âƒ£ ë²¡í„° ìƒì„± (text-embedding-ada-002)
    const embeddingResponse = await openai.embeddings.create({
      model: "text-embedding-3-small",
      input: summaryParser(summary), // ì„œë¨¸ë¦¬ë¥¼ ë²¡í„°í™”
    });

    const vector = embeddingResponse.data[0]?.embedding || [];

    return NextResponse.json({ summary, vector });
  } catch (error) {
    console.error("OpenAI API ì˜¤ë¥˜:", error);
    return NextResponse.json(
      { error: "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ" },
      { status: 500 }
    );
  }
}

const MAX_TOKENS = 600;

const MARKDOWN_SUMMARY = `
## âœ¨ í•œëˆˆì— ë³´ëŠ” ìš”ì•½
Next.js 14ì˜ App Routerë¥¼ í™œìš©í•˜ë©´ í´ë” êµ¬ì¡°ë§Œìœ¼ë¡œ ì§ê´€ì ì¸ ë¼ìš°íŒ…ì„ êµ¬ì„±í•  ìˆ˜ ìˆì–´ìš”.  
ì´ ê¸€ì—ì„œëŠ” Supabaseì™€ ì—°ë™í•´ ì„œë²„ ì—†ì´ë„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê³ ,  
Zustandë¥¼ í™œìš©í•´ í´ë¼ì´ì–¸íŠ¸ ìƒíƒœë¥¼ ê¹”ë”í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” íŒì„ ì†Œê°œí•©ë‹ˆë‹¤.  
ë˜í•œ, Next.jsì˜ ì„œë²„ ì»´í¬ë„ŒíŠ¸ë¥¼ í™œìš©í•´ **ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì›¹ ì•±**ì„ ë§Œë“œëŠ” ë°©ë²•ë„ ë‹¤ë¤„ìš”. ğŸš€  

---

## ğŸ”¥ í•µì‹¬ í¬ì¸íŠ¸
- **App Routerì˜ ê°•ì ** â†’ í´ë” ê¸°ë°˜ ë¼ìš°íŒ…ê³¼ SSRì„ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆì–´ìš”.  
- **Supabaseì™€ ë°ì´í„° ê´€ë¦¬** â†’ ì¸ì¦ & ì‹¤ì‹œê°„ ë°ì´í„° ì—°ë™ê¹Œì§€ í•œ ë²ˆì—!  
- **Zustand í™œìš©ë²•** â†’ ê°€ë²¼ìš°ë©´ì„œë„ ê°•ë ¥í•œ ê¸€ë¡œë²Œ ìƒíƒœ ê´€ë¦¬  
- **Next.js ì„±ëŠ¥ ìµœì í™”** â†’ ì„œë²„ ì»´í¬ë„ŒíŠ¸ì™€ ìºì‹±ì„ ì œëŒ€ë¡œ í™œìš©í•´ë³´ì.  

---

## ğŸ“š ë” ê³µë¶€í•˜ë©´ ì¢‹ì€ ì£¼ì œ
1. **React Server Components (RSC)** â€“ ì„œë²„ì—ì„œ ë Œë”ë§ì„ ìµœì í™”í•˜ëŠ” ìµœì‹  íŠ¸ë Œë“œ  
2. **Edge Functions** â€“ Vercelì„ í™œìš©í•´ ë” ë¹ ë¥¸ API ì‘ë‹µ ë°›ê¸°  
3. **Database Indexing & Optimization** â€“ Supabaseì—ì„œ ì¿¼ë¦¬ ì„±ëŠ¥ ë†’ì´ê¸°  
4. **React Query vs Zustand** â€“ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ ìƒíƒœ ê´€ë¦¬ê°€ ë” ì¢‹ì„ê¹Œ?  
5. **Vercel & Cloudflare Optimization** â€“ Next.js ë°°í¬ í›„ ì†ë„ ìµœì í™”í•˜ëŠ” ë²• ğŸš€  
`;
